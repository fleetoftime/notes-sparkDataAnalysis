{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "sc = SparkContext(\"local\",\"WordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filePath = os.path.abspath(\"./quickstart.txt\")\n",
    "content = sc.textFile(\"file://\" + filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "('', 71)\n",
      "('Spark’s', 3)\n",
      "('sets,', 1)\n",
      "('name', 1)\n",
      "('Dataset', 1)\n",
      "('popularized', 1)\n",
      "('how', 1)\n",
      "('PageRank.', 1)\n",
      "('for', 4)\n",
      "('tens', 1)\n",
      "('SimpleApp', 1)\n",
      "('wordCounts.collect()', 1)\n",
      "('depends', 2)\n",
      "('make', 2)\n",
      "('Go', 2)\n",
      "('then', 2)\n",
      "('location', 1)\n",
      "('directory:', 2)\n",
      "('Packaging', 1)\n",
      "('SimpleApp.scala', 2)\n",
      "('Python.', 3)\n",
      "('return', 3)\n",
      "('1)).reduceByKey((a,', 1)\n",
      "('so', 1)\n",
      "('algorithm', 1)\n",
      "('else', 1)\n",
      "('as', 9)\n",
      "('any', 2)\n",
      "('used', 2)\n",
      "('./src/main', 1)\n",
      "('you', 1)\n",
      "('\")).map(word', 1)\n",
      "('Use', 1)\n",
      "('text', 2)\n",
      "('several', 1)\n",
      "('easier', 1)\n",
      "('textFile:', 1)\n",
      "('where', 1)\n",
      "('by', 4)\n",
      "('SparkContext', 2)\n",
      "('scala>', 13)\n",
      "('head', 1)\n",
      "('\"Spark\"?', 1)\n",
      "('easily', 1)\n",
      "('simple.sbt,', 1)\n",
      "('wish', 1)\n",
      "('replace', 1)\n",
      "('constructor', 1)\n",
      "('Package', 1)\n",
      "('logData.filter(line', 2)\n",
      "('libraryDependencies', 1)\n",
      "('line.split(\"', 3)\n",
      "('We', 5)\n",
      "('may', 2)\n",
      "('Operations', 2)\n",
      "('work', 2)\n",
      "('More', 2)\n",
      "('largest', 1)\n",
      "('cache.', 1)\n",
      "('repeatedly,', 1)\n",
      "('version', 2)\n",
      "('includes', 1)\n",
      "('shell,', 2)\n",
      "('MapReduce', 1)\n",
      "('show', 1)\n",
      "('(which', 1)\n",
      "('Should', 1)\n",
      "('spark.FilteredRDD@7dd4af09', 1)\n",
      "('Python,', 1)\n",
      "('org.apache.spark.SparkConf', 1)\n",
      "('15', 3)\n",
      "('cache', 1)\n",
      "('functions', 2)\n",
      "('new', 6)\n",
      "('As', 1)\n",
      "('source', 1)\n",
      "('libraries)', 1)\n",
      "('adds', 1)\n",
      "('subset', 1)\n",
      "('org.apache.spark.SparkContext._', 1)\n",
      "('100-line', 1)\n",
      "('flatMap,', 1)\n",
      "('res7:', 1)\n",
      "('sbt', 3)\n",
      "('+', 1)\n",
      "('configuration', 1)\n",
      "('%s\".format(numAs,', 1)\n",
      "('action:', 1)\n",
      "('Congratulations', 1)\n",
      "('b))', 1)\n",
      "('system', 1)\n",
      "('Hadoop', 1)\n",
      "('.', 2)\n",
      "('RDD.', 1)\n",
      "('(Scala,', 1)\n",
      "('res1:', 1)\n",
      "('map', 1)\n",
      "('Self-Contained', 2)\n",
      "('way', 2)\n",
      "('def', 1)\n",
      "('java.lang.Math', 2)\n",
      "('linesWithSpark.count()', 2)\n",
      "('Scala', 8)\n",
      "('download', 2)\n",
      "('together', 1)\n",
      "('interactive', 1)\n",
      "('println(\"Lines', 1)\n",
      "('Finally,', 1)\n",
      "('--class', 1)\n",
      "('is', 11)\n",
      "('(Because,1),', 1)\n",
      "('dataset', 2)\n",
      "('are', 2)\n",
      "('useful', 1)\n",
      "('or', 7)\n",
      "('%', 1)\n",
      "('seem', 1)\n",
      "('these', 1)\n",
      "('res9:', 1)\n",
      "('wordCounts', 1)\n",
      "('Array[(String,', 1)\n",
      "('Hadoop.', 2)\n",
      "('textFile.filter(line', 2)\n",
      "('‘b’', 1)\n",
      "('program', 1)\n",
      "('Interactive', 2)\n",
      "('such', 1)\n",
      "('jar', 1)\n",
      "('word', 1)\n",
      "('from', 5)\n",
      "('Caching', 2)\n",
      "('well', 1)\n",
      "('can', 12)\n",
      "('using', 3)\n",
      "('HDFS,', 1)\n",
      "('along', 1)\n",
      "('Java', 4)\n",
      "('Math.max()', 1)\n",
      "('flow', 1)\n",
      "('we’ll', 2)\n",
      "('spark.RDD[(String,', 1)\n",
      "('thus', 1)\n",
      "('our', 4)\n",
      "('filter', 1)\n",
      "('introduction', 1)\n",
      "('$', 3)\n",
      "('many', 1)\n",
      "('either', 1)\n",
      "('Here,', 1)\n",
      "('directly:', 2)\n",
      "('Guides”', 1)\n",
      "('large', 1)\n",
      "('object', 2)\n",
      "('interactively', 1)\n",
      "('textFile.flatMap(line', 1)\n",
      "('126', 1)\n",
      "('powerful', 1)\n",
      "('simple.sbt', 1)\n",
      "('won’t', 1)\n",
      "('You', 2)\n",
      "('YOUR_SPARK_HOME', 1)\n",
      "('dependency.', 1)\n",
      "('quick', 1)\n",
      "('textFile.map(line', 2)\n",
      "('running', 4)\n",
      "('analyze', 1)\n",
      "('website.', 1)\n",
      "('(Python,2),', 1)\n",
      "('same', 1)\n",
      "('API', 1)\n",
      "('{..}/{..}/target/scala-2.10/simple-project_2.10-1.0.jar', 1)\n",
      "('application’s', 1)\n",
      "('easily:', 1)\n",
      "('provides', 2)\n",
      "('repository', 1)\n",
      "('shell', 2)\n",
      "('arguments', 1)\n",
      "('walk', 1)\n",
      "('like', 2)\n",
      "('in-depth', 1)\n",
      "('in', 16)\n",
      "('querying', 1)\n",
      "('see', 1)\n",
      "('textFile.first()', 1)\n",
      "('./src/main/scala', 1)\n",
      "('line', 3)\n",
      "('runs', 1)\n",
      "('(in', 1)\n",
      "('19', 2)\n",
      "('interesting', 1)\n",
      "('b:', 2)\n",
      "('structure.', 1)\n",
      "('lines', 2)\n",
      "('need', 2)\n",
      "('should', 2)\n",
      "('Suppose', 1)\n",
      "('counts', 3)\n",
      "('Maven),', 1)\n",
      "('words:', 1)\n",
      "('\"1.6.2\"', 1)\n",
      "('Scala/Java', 1)\n",
      "('say', 1)\n",
      "('linesWithSpark:', 1)\n",
      "('simple', 4)\n",
      "('Analysis', 2)\n",
      "('distributed', 1)\n",
      "('files)', 1)\n",
      "('pass', 1)\n",
      "('(closures),', 1)\n",
      "('spark.RDD[String]', 3)\n",
      "('Math.max(a,', 1)\n",
      "('code', 1)\n",
      "('line.contains(\"a\")).count()', 1)\n",
      "('res4:', 1)\n",
      "('deployment', 1)\n",
      "('cached:', 1)\n",
      "('(word,', 1)\n",
      "('Now', 1)\n",
      "('function', 2)\n",
      "('of', 13)\n",
      "('Basics', 2)\n",
      "('available', 1)\n",
      "('Our', 1)\n",
      "('map,', 1)\n",
      "('textFile', 1)\n",
      "('23', 1)\n",
      "('them', 1)\n",
      "('complex', 1)\n",
      "('application', 6)\n",
      "('write', 2)\n",
      "('Since', 1)\n",
      "('define', 1)\n",
      "('textFile.count()', 1)\n",
      "('in-memory', 1)\n",
      "('\"spark-core\"', 1)\n",
      "('follows:', 1)\n",
      "('computations.', 1)\n",
      "('on', 9)\n",
      "('an', 5)\n",
      "('fact,', 1)\n",
      "('Start', 2)\n",
      "('API.', 1)\n",
      "('pattern', 1)\n",
      "('RDD', 9)\n",
      "('very', 3)\n",
      "('good', 1)\n",
      "('pulling', 1)\n",
      "('String', 1)\n",
      "('number', 2)\n",
      "('feature', 1)\n",
      "('extending', 1)\n",
      "('Quick', 1)\n",
      "('RDDs.', 2)\n",
      "('Unlike', 1)\n",
      "('elsewhere.', 1)\n",
      "('Int)', 1)\n",
      "('value,', 1)\n",
      "('iterative', 1)\n",
      "('Spark', 21)\n",
      "('line.contains(\"Spark\")).count()', 1)\n",
      "('MapReduce,', 1)\n",
      "('item', 1)\n",
      "('primary', 1)\n",
      "('Project\"', 1)\n",
      "('//', 4)\n",
      "('line.contains(\"b\")).count()', 1)\n",
      "('numBs))', 1)\n",
      "('%%', 1)\n",
      "('guide,', 2)\n",
      "('spark.MappedRDD@2ee9b6e3', 1)\n",
      "('we', 8)\n",
      "('combined', 1)\n",
      "('per-word', 1)\n",
      "('(a', 1)\n",
      "('Scala),', 1)\n",
      "('a:', 2)\n",
      "('We’ll', 2)\n",
      "('not', 1)\n",
      "('sc.textFile(logFile,', 1)\n",
      "('they', 1)\n",
      "('about', 1)\n",
      "('(cluster.,1),', 1)\n",
      "('import', 5)\n",
      "('want', 1)\n",
      "('res3:', 1)\n",
      "('striped', 1)\n",
      "('Let’s', 3)\n",
      "('SimpleApp.scala:', 1)\n",
      "('main()', 1)\n",
      "('cluster,', 2)\n",
      "('let’s', 2)\n",
      "('compute', 1)\n",
      "('release', 1)\n",
      "('pairs.', 1)\n",
      "('named', 1)\n",
      "('(this,3),', 1)\n",
      "('data', 5)\n",
      "('package', 3)\n",
      "('}', 2)\n",
      "('tool', 1)\n",
      "('tutorial', 1)\n",
      "('overview.', 1)\n",
      "('The', 2)\n",
      "('reduceByKey', 1)\n",
      "('programming', 3)\n",
      "('find', 3)\n",
      "('a', 34)\n",
      "('sc.textFile(\"README.md\")', 1)\n",
      "('include', 1)\n",
      "('sbt),', 1)\n",
      "('scala.App', 1)\n",
      "('menu', 1)\n",
      "('sets', 1)\n",
      "('JAR', 1)\n",
      "('with', 13)\n",
      "('call', 1)\n",
      "('/*', 1)\n",
      "('spark.FilteredRDD@17e51082', 1)\n",
      "('R', 1)\n",
      "('explains', 1)\n",
      "('--master', 1)\n",
      "('transformations,', 1)\n",
      "('contain', 1)\n",
      "('creating', 1)\n",
      "('actions:', 2)\n",
      "('README', 1)\n",
      "('Int)]', 2)\n",
      "('file', 4)\n",
      "('Resilient', 1)\n",
      "('\"org.apache.spark\"', 1)\n",
      "('See', 1)\n",
      "('(under,2),', 1)\n",
      "('#', 7)\n",
      "('\"YOUR_SPARK_HOME/README.md\"', 1)\n",
      "('examples,', 2)\n",
      "('chain', 1)\n",
      "('=', 21)\n",
      "('introduce', 1)\n",
      "('even', 1)\n",
      "('2).cache()', 1)\n",
      "('Python', 6)\n",
      "('examples/src/main/python/pi.py', 1)\n",
      "('org.apache.spark.SparkContext', 1)\n",
      "('Spark.', 1)\n",
      "('InputFormats', 1)\n",
      "('complete', 1)\n",
      "('\"Simple', 1)\n",
      "('own', 1)\n",
      "('val', 9)\n",
      "('interactively.', 1)\n",
      "('res8:', 1)\n",
      "('./bin/spark-shell', 1)\n",
      "('this', 6)\n",
      "('\"2.10.5\"', 1)\n",
      "('linesWithSpark.cache()', 1)\n",
      "('Lines', 3)\n",
      "('Where', 2)\n",
      "('example,', 2)\n",
      "('For', 7)\n",
      "('more', 2)\n",
      "('understand:', 1)\n",
      "('[info]', 1)\n",
      "('called', 2)\n",
      "('./src/main/scala/SimpleApp.scala', 1)\n",
      "('containing', 4)\n",
      "('file.', 2)\n",
      "('packaged', 1)\n",
      "('method', 1)\n",
      "('To', 2)\n",
      "('connecting', 1)\n",
      "('which', 5)\n",
      "('%s,', 1)\n",
      "('res6:', 1)\n",
      "('mark', 1)\n",
      "('applications', 3)\n",
      "('“Programming', 1)\n",
      "('accessed', 1)\n",
      "('This', 5)\n",
      "('Applications', 2)\n",
      "('application.', 1)\n",
      "('“hot”', 1)\n",
      "('other', 2)\n",
      "('flows', 1)\n",
      "('use', 11)\n",
      "('./bin/run-example', 1)\n",
      "('(RDD).', 1)\n",
      "('Subclasses', 1)\n",
      "('‘a’', 1)\n",
      "('spark-submit', 4)\n",
      "('small', 1)\n",
      "('self-contained', 1)\n",
      "('...)', 1)\n",
      "('scala.App.', 1)\n",
      "('earlier', 1)\n",
      "('res5:', 1)\n",
      "('created', 1)\n",
      "('Note', 2)\n",
      "('simple,', 1)\n",
      "('transformation', 1)\n",
      "('(with', 2)\n",
      "('{', 2)\n",
      "('values,', 1)\n",
      "('guide', 1)\n",
      "('SparkContext(conf)', 1)\n",
      "('numAs', 1)\n",
      "('...', 2)\n",
      "('run-example:', 1)\n",
      "('Scala–so', 1)\n",
      "('It', 2)\n",
      "('Here', 2)\n",
      "('place,', 1)\n",
      "('installed.', 1)\n",
      "('its', 1)\n",
      "('correctly,', 1)\n",
      "('the', 41)\n",
      "('it’s', 1)\n",
      "('components.', 1)\n",
      "('hundreds', 1)\n",
      "('it', 1)\n",
      "('Int', 1)\n",
      "('first', 4)\n",
      "('following', 1)\n",
      "('RDDs', 2)\n",
      "('logData', 1)\n",
      "('API,', 3)\n",
      "('(such', 1)\n",
      "('instead', 1)\n",
      "('silly', 1)\n",
      "('just', 1)\n",
      "('Scala,', 1)\n",
      "('transformations', 3)\n",
      "('program.', 2)\n",
      "('count.', 1)\n",
      "('described', 1)\n",
      "('guide.', 1)\n",
      "('that', 8)\n",
      "('script', 1)\n",
      "('scalaVersion', 1)\n",
      "('\\\\', 3)\n",
      "('have', 1)\n",
      "('and', 13)\n",
      "('Shell', 2)\n",
      "('some', 1)\n",
      "('common', 1)\n",
      "('actions', 1)\n",
      "('\").size).reduce((a,', 2)\n",
      "('through', 2)\n",
      "('conf', 1)\n",
      "('line.contains(\"Spark\"))', 1)\n",
      "('language', 1)\n",
      "('correctly.', 1)\n",
      "(':=', 3)\n",
      "('to', 28)\n",
      "('Array[String])', 1)\n",
      "('Array((means,1),', 1)\n",
      "('if', 1)\n",
      "('code,', 1)\n",
      "('./src', 1)\n",
      "('./simple.sbt', 1)\n",
      "('collection', 1)\n",
      "('most', 1)\n",
      "('reference.', 1)\n",
      "('supports', 1)\n",
      "('initializes', 1)\n",
      "('linesWithSpark', 2)\n",
      "('Application\")', 1)\n",
      "('transforming', 1)\n",
      "('\"1.0\"', 1)\n",
      "('How', 1)\n",
      "('library.', 1)\n",
      "('samples', 1)\n",
      "('Your', 1)\n",
      "('(String,', 1)\n",
      "('Once', 1)\n",
      "('Number', 1)\n",
      "('Distributed', 1)\n",
      "('target/scala-2.10/simple-project_2.10-1.0.jar', 1)\n",
      "('numBs', 1)\n",
      "('will', 3)\n",
      "('part', 2)\n",
      "('into', 1)\n",
      "('layout', 2)\n",
      "('your', 4)\n",
      "('+=', 1)\n",
      "('46,', 1)\n",
      "('start', 2)\n",
      "('on:', 1)\n",
      "('few', 1)\n",
      "('main(args:', 1)\n",
      "('items', 3)\n",
      "('=>', 11)\n",
      "('declared', 1)\n",
      "('file,', 1)\n",
      "('follow', 1)\n",
      "('YOUR_SPARK_HOME/bin/spark-submit', 1)\n",
      "('you’ll', 1)\n",
      "('logFile', 1)\n",
      "('abstraction', 1)\n",
      "('reduce', 2)\n",
      "('be', 6)\n",
      "('literals', 1)\n",
      "('bin/spark-shell', 1)\n",
      "('One', 1)\n",
      "('existing', 1)\n",
      "('actions,', 1)\n",
      "('explore', 1)\n",
      "('nodes.', 1)\n",
      "('sc', 1)\n",
      "('when', 4)\n",
      "('cluster-wide', 1)\n",
      "('\"SimpleApp\"', 1)\n",
      "('do', 1)\n",
      "('First', 1)\n",
      "('VM', 1)\n",
      "('SparkConf().setAppName(\"Simple', 1)\n",
      "('transformation.', 1)\n",
      "('according', 1)\n",
      "('Apache', 1)\n",
      "('maps', 1)\n",
      "('README.', 1)\n",
      "('create', 2)\n",
      "('Long', 5)\n",
      "('R).', 1)\n",
      "('directory', 3)\n",
      "('application!', 1)\n",
      "('across', 1)\n",
      "('*/', 1)\n",
      "('examples', 2)\n",
      "('collect', 2)\n",
      "('Java,', 3)\n",
      "('initialize', 1)\n",
      "('b)', 6)\n",
      "('pointers', 1)\n",
      "('examples/src/main/r/dataframe.R', 1)\n",
      "('learn', 1)\n",
      "('./bin/spark-submit', 2)\n",
      "('contains', 1)\n",
      "('typical', 1)\n",
      "('>', 1)\n",
      "('information', 1)\n",
      "('wordCounts:', 1)\n",
      "('(agree,1),', 1)\n",
      "('run', 3)\n",
      "('SparkContext,', 1)\n",
      "('res0:', 1)\n",
      "('HDFS', 1)\n",
      "('implement', 1)\n",
      "('SparkConf', 1)\n",
      "('look', 1)\n",
      "('also', 4)\n",
      "('local[4]', 1)\n",
      "('integer', 1)\n",
      "('overview', 1)\n",
      "('SparkPi', 1)\n",
      "('spark.ShuffledAggregatedRDD@71f027b8', 1)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/lixiwei-mac/Documents/python/notes-sparkDataAnalysis/wordcount.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5be2162d222a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///Users/lixiwei-mac/Documents/python/notes-sparkDataAnalysis/wordcount.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lixiwei-mac/app/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lixiwei-mac/anaconda/lib/python3.5/site-packages/py4j-0.9.1-py3.5.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 835\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lixiwei-mac/anaconda/lib/python3.5/site-packages/py4j-0.9.1-py3.5.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    309\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/lixiwei-mac/Documents/python/notes-sparkDataAnalysis/wordcount.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "words = content.flatMap(lambda line : line.split(' '))\n",
    "print(type(words))\n",
    "print(type(words.map(lambda word : (word,1))))\n",
    "counts = words.map(lambda word : (word,1)).reduceByKey(lambda x,y : x + y)\n",
    "for d in counts.collect():\n",
    "    print(d)\n",
    "counts.saveAsTextFile(\"file:///Users/lixiwei-mac/Documents/python/notes-sparkDataAnalysis/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lixiwei-mac/Documents/python/notes-sparkDataAnalysis\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

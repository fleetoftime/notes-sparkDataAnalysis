{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter11.SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "sc = SparkContext(\"local[*]\",\"SparkML\")\n",
    "spam = sc.textFile(\"file:///Users/lixiwei-mac/Documents/git/learning-spark/files/rubbish.txt\")\n",
    "normal = sc.textFile(\"file:///Users/lixiwei-mac/Documents/git/learning-spark/files/normal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(10000, {830: 1.0, 3289: 1.0, 3870: 1.0, 5542: 1.0, 6299: 1.0}), SparseVector(10000, {5542: 4.0}), SparseVector(10000, {2744: 1.0, 5542: 1.0, 5706: 1.0}), SparseVector(10000, {0: 1.0, 1363: 1.0, 5542: 1.0, 6554: 1.0, 9466: 1.0, 9740: 1.0}), SparseVector(10000, {0: 1.0}), SparseVector(10000, {2403: 1.0, 3233: 1.0, 3870: 1.0, 5542: 1.0})]\n",
      "[SparseVector(10000, {830: 1.0, 3289: 1.0, 3870: 1.0, 4842: 1.0, 6931: 1.0}), SparseVector(10000, {2678: 1.0, 2744: 1.0, 6931: 1.0, 9241: 1.0}), SparseVector(10000, {1499: 1.0, 1518: 1.0, 1583: 1.0, 2403: 1.0, 2744: 1.0, 3317: 1.0, 3870: 1.0, 4757: 1.0, 6554: 1.0, 6931: 1.0}), SparseVector(10000, {1136: 1.0, 2438: 1.0, 6052: 1.0, 6931: 1.0, 8977: 1.0}), SparseVector(10000, {6931: 4.0})]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个HashingTF实例来把文本映射为包含10000个特征的向量\n",
    "tf = HashingTF(numFeatures=10000)\n",
    "# 各邮件都被切分为单词，每个单词都被映射为一个特征\n",
    "spamFeatures = spam.map(lambda email : tf.transform(email.split(\" \")))\n",
    "print(spamFeatures.collect())\n",
    "normalFeatures = normal.map(lambda email : tf.transform(email.split(\" \")))\n",
    "print(normalFeatures.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[8] at union at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建LabeledPoint数据集分别存放 垃圾邮件 和 正常邮件 的例子\n",
    "postiveExamples = spamFeatures.map(lambda features : LabeledPoint(0,features))\n",
    "negativeExamples = normalFeatures.map(lambda features : LabeledPoint(1,features))\n",
    "trainingData = postiveExamples.union(negativeExamples)\n",
    "trainingData.cache()# 因为逻辑回归是迭代算法，所以缓存训练数据RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用SGD算法运行逻辑回归\n",
    "model = LogisticRegressionWithSGD.train(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for postive test example: 0\n",
      "Prediction for negative test example: 0\n"
     ]
    }
   ],
   "source": [
    "# 以 垃圾邮件 和 正常邮件 的例子分别进行测试\n",
    "# 首先使用一样的HashingTF特征来得到特征向量，然后对该向量应用得到的模型\n",
    "posTest = tf.transform(\"rubbish rubbish rubbish rubbish rubbish rubbish rubbish rubbish \".split())\n",
    "negTest = tf.transform(\"normal normal normal normal normal normal normal\".split())\n",
    "print(\"Prediction for postive test example: %g\" % model.predict(posTest))\n",
    "print(\"Prediction for negative test example: %g\" % model.predict(negTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.数据类型\n",
    "- 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  0.]\n",
      "[1.0,2.0,3.0,0.0]\n",
      "(4,[0,1,3],[1.0,2.0,0.0])\n",
      "(4,[0,2,5],[1.0,2.0,0.0])\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "# 创建稠密向量 <1.0,2.0,3.0>\n",
    "denseVec1 = array([1.0,2.0,3.0,0.0]) # Numpy 数组可以直接传给Mllib\n",
    "denseVec2 = Vectors.dense([1.0,2.0,3.0,0.0]) # 或者使用Vectors类来创建\n",
    "print(denseVec1)\n",
    "print(denseVec2)\n",
    "# 创建稀疏向量 该方法直接收向量的维度以及非0位的位置和对应的值\n",
    "# 这些数据可以用一个dict来传递，或者使用两个分别代表位置和值的list\n",
    "sparseVec1 = Vectors.sparse(4,{0:1.0,1:2.0,3:0})\n",
    "sparseVec2 = Vectors.sparse(4,[0,2,5],[1.0,2.0,0.0])\n",
    "print(sparseVec1)\n",
    "print(sparseVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(10000, {5947: 0.0, 9861: 0.0})]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from pyspark.mllib.feature import HashingTF,IDF\n",
    "sentence = \"hello hello world\"\n",
    "rdd = sc.wholeTextFiles(\"file:///Users/lixiwei-mac/Documents/git/learning-spark/files/rubbish.txt\")\n",
    "words = sentence.split()\n",
    "tf = HashingTF(10000)\n",
    "tfVectors = tf.transform(rdd).cache()\n",
    "\n",
    "# 计算IDF,然后计算TF-IDF向量\n",
    "idf = IDF()\n",
    "idfModel = idf.fit(tfVectors)\n",
    "tfIdfVectors = idfModel.transform(tfVectors)\n",
    "print(tfIdfVectors.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缩放\n",
    "控制平均值 和 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.7071, -0.7071, -0.7071]),\n",
       " DenseVector([0.7071, 0.7071, 0.7071])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler,Vectors,Normalizer,Word2Vec\n",
    "vectors = [Vectors.dense([1,2,3]),Vectors.dense([5,6,7])]\n",
    "dataset = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True,withStd=True)\n",
    "model = scaler.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正规化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-fdfd08908ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'vector'"
     ]
    }
   ],
   "source": [
    "Normalizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9873ebfc60b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"123\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类与回归\n",
    "- 线性回归\n",
    "- 逻辑回归\n",
    "- 支持向量机\n",
    "- 朴素贝叶斯\n",
    "- 决策树与随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "points = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

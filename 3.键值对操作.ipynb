{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 键值对操作   \n",
    "### 1.动机\n",
    "- pairRDD: 提供并行操作各个键或跨节点重新进行数据分组的操作接口。\n",
    "- reduceByKey: 分别规约每个键对应的数据\n",
    "- join: 可以把两个RDD中键相同的的元素组合到一起，合并为一个RDD。\n",
    "\n",
    "### 2.创建PairRDD\n",
    "```\n",
    "rawRDD = sc.parallelize([\"打招呼 你好啊~~吃了吗??\",\"约 约吗美女?\"])\n",
    "pairRDD = rawRDD.map(lambda line : (line.split(' ')[0],line.split(' ')[1:]))\n",
    "pairRDD.collect()\n",
    "```\n",
    "### 3.PairRDD的转化操作\n",
    "#### 0.基本函数操作详见代码\n",
    "略\n",
    "#### 1.聚合操作   \n",
    "- 基础RDD聚合操作: reduce(),combine(),fold()\n",
    "- PairRDD聚合操作: reduceByKey().combineByKey(),foldByKey()\n",
    "- 数据分组: groupByKey().    groupByKey + mapValues 可以实现reduceByKey()同样功能，但前者效率低\n",
    "- 连接: join(),leftOuterJoin(),rightOuterJoin().\n",
    "- 数据排序: sortByKey()    \n",
    "### 4.PairRDD的行动操作\n",
    "- countByKey(): 对每个键嘴硬的元素分别计数\n",
    "- countByValue(): 按键对值进行统计  单词计数简化版\n",
    "- collectAsMap(): 结果以映射表的形式返回\n",
    "- lookup(key): 返回给定键对应的所有值 \n",
    "### 5.数据分区（进阶）\n",
    "使用自定义分区来提高效率，减少每次对不变的表进行混洗操作而消耗时间。例如使用rdd.partitionBy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext(\"local\",\"PairRDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'': 71,\n",
       "             '\")).map(word': 1,\n",
       "             '\").size).reduce((a,': 2,\n",
       "             '\"1.0\"': 1,\n",
       "             '\"1.6.2\"': 1,\n",
       "             '\"2.10.5\"': 1,\n",
       "             '\"Simple': 1,\n",
       "             '\"SimpleApp\"': 1,\n",
       "             '\"Spark\"?': 1,\n",
       "             '\"YOUR_SPARK_HOME/README.md\"': 1,\n",
       "             '\"org.apache.spark\"': 1,\n",
       "             '\"spark-core\"': 1,\n",
       "             '#': 7,\n",
       "             '$': 3,\n",
       "             '%': 1,\n",
       "             '%%': 1,\n",
       "             '%s\".format(numAs,': 1,\n",
       "             '%s,': 1,\n",
       "             '(Because,1),': 1,\n",
       "             '(Python,2),': 1,\n",
       "             '(RDD).': 1,\n",
       "             '(Scala,': 1,\n",
       "             '(String,': 1,\n",
       "             '(a': 1,\n",
       "             '(agree,1),': 1,\n",
       "             '(closures),': 1,\n",
       "             '(cluster.,1),': 1,\n",
       "             '(in': 1,\n",
       "             '(such': 1,\n",
       "             '(this,3),': 1,\n",
       "             '(under,2),': 1,\n",
       "             '(which': 1,\n",
       "             '(with': 2,\n",
       "             '(word,': 1,\n",
       "             '*/': 1,\n",
       "             '+': 1,\n",
       "             '+=': 1,\n",
       "             '--class': 1,\n",
       "             '--master': 1,\n",
       "             '.': 2,\n",
       "             '...': 2,\n",
       "             '...)': 1,\n",
       "             './bin/run-example': 1,\n",
       "             './bin/spark-shell': 1,\n",
       "             './bin/spark-submit': 2,\n",
       "             './simple.sbt': 1,\n",
       "             './src': 1,\n",
       "             './src/main': 1,\n",
       "             './src/main/scala': 1,\n",
       "             './src/main/scala/SimpleApp.scala': 1,\n",
       "             '/*': 1,\n",
       "             '//': 4,\n",
       "             '1)).reduceByKey((a,': 1,\n",
       "             '100-line': 1,\n",
       "             '126': 1,\n",
       "             '15': 3,\n",
       "             '19': 2,\n",
       "             '2).cache()': 1,\n",
       "             '23': 1,\n",
       "             '46,': 1,\n",
       "             ':=': 3,\n",
       "             '=': 21,\n",
       "             '=>': 11,\n",
       "             '>': 1,\n",
       "             'API': 1,\n",
       "             'API,': 3,\n",
       "             'API.': 1,\n",
       "             'Analysis': 2,\n",
       "             'Apache': 1,\n",
       "             'Application\")': 1,\n",
       "             'Applications': 2,\n",
       "             'Array((means,1),': 1,\n",
       "             'Array[(String,': 1,\n",
       "             'Array[String])': 1,\n",
       "             'As': 1,\n",
       "             'Basics': 2,\n",
       "             'Caching': 2,\n",
       "             'Congratulations': 1,\n",
       "             'Dataset': 1,\n",
       "             'Distributed': 1,\n",
       "             'Finally,': 1,\n",
       "             'First': 1,\n",
       "             'For': 7,\n",
       "             'Go': 2,\n",
       "             'Guides”': 1,\n",
       "             'HDFS': 1,\n",
       "             'HDFS,': 1,\n",
       "             'Hadoop': 1,\n",
       "             'Hadoop.': 2,\n",
       "             'Here': 2,\n",
       "             'Here,': 1,\n",
       "             'How': 1,\n",
       "             'InputFormats': 1,\n",
       "             'Int': 1,\n",
       "             'Int)': 1,\n",
       "             'Int)]': 2,\n",
       "             'Interactive': 2,\n",
       "             'It': 2,\n",
       "             'JAR': 1,\n",
       "             'Java': 4,\n",
       "             'Java,': 3,\n",
       "             'Let’s': 3,\n",
       "             'Lines': 3,\n",
       "             'Long': 5,\n",
       "             'MapReduce': 1,\n",
       "             'MapReduce,': 1,\n",
       "             'Math.max()': 1,\n",
       "             'Math.max(a,': 1,\n",
       "             'Maven),': 1,\n",
       "             'More': 2,\n",
       "             'Note': 2,\n",
       "             'Now': 1,\n",
       "             'Number': 1,\n",
       "             'Once': 1,\n",
       "             'One': 1,\n",
       "             'Operations': 2,\n",
       "             'Our': 1,\n",
       "             'Package': 1,\n",
       "             'Packaging': 1,\n",
       "             'PageRank.': 1,\n",
       "             'Project\"': 1,\n",
       "             'Python': 6,\n",
       "             'Python,': 1,\n",
       "             'Python.': 3,\n",
       "             'Quick': 1,\n",
       "             'R': 1,\n",
       "             'R).': 1,\n",
       "             'RDD': 9,\n",
       "             'RDD.': 1,\n",
       "             'RDDs': 2,\n",
       "             'RDDs.': 2,\n",
       "             'README': 1,\n",
       "             'README.': 1,\n",
       "             'Resilient': 1,\n",
       "             'Scala': 8,\n",
       "             'Scala),': 1,\n",
       "             'Scala,': 1,\n",
       "             'Scala/Java': 1,\n",
       "             'Scala–so': 1,\n",
       "             'See': 1,\n",
       "             'Self-Contained': 2,\n",
       "             'Shell': 2,\n",
       "             'Should': 1,\n",
       "             'SimpleApp': 1,\n",
       "             'SimpleApp.scala': 2,\n",
       "             'SimpleApp.scala:': 1,\n",
       "             'Since': 1,\n",
       "             'Spark': 21,\n",
       "             'Spark.': 1,\n",
       "             'SparkConf': 1,\n",
       "             'SparkConf().setAppName(\"Simple': 1,\n",
       "             'SparkContext': 2,\n",
       "             'SparkContext(conf)': 1,\n",
       "             'SparkContext,': 1,\n",
       "             'SparkPi': 1,\n",
       "             'Spark’s': 3,\n",
       "             'Start': 2,\n",
       "             'String': 1,\n",
       "             'Subclasses': 1,\n",
       "             'Suppose': 1,\n",
       "             'The': 2,\n",
       "             'This': 5,\n",
       "             'To': 2,\n",
       "             'Unlike': 1,\n",
       "             'Use': 1,\n",
       "             'VM': 1,\n",
       "             'We': 5,\n",
       "             'We’ll': 2,\n",
       "             'Where': 2,\n",
       "             'YOUR_SPARK_HOME': 1,\n",
       "             'YOUR_SPARK_HOME/bin/spark-submit': 1,\n",
       "             'You': 2,\n",
       "             'Your': 1,\n",
       "             '[info]': 1,\n",
       "             '\\\\': 3,\n",
       "             'a': 34,\n",
       "             'a:': 2,\n",
       "             'about': 1,\n",
       "             'abstraction': 1,\n",
       "             'accessed': 1,\n",
       "             'according': 1,\n",
       "             'across': 1,\n",
       "             'action:': 1,\n",
       "             'actions': 1,\n",
       "             'actions,': 1,\n",
       "             'actions:': 2,\n",
       "             'adds': 1,\n",
       "             'algorithm': 1,\n",
       "             'along': 1,\n",
       "             'also': 4,\n",
       "             'an': 5,\n",
       "             'analyze': 1,\n",
       "             'and': 13,\n",
       "             'any': 2,\n",
       "             'application': 6,\n",
       "             'application!': 1,\n",
       "             'application.': 1,\n",
       "             'applications': 3,\n",
       "             'application’s': 1,\n",
       "             'are': 2,\n",
       "             'arguments': 1,\n",
       "             'as': 9,\n",
       "             'available': 1,\n",
       "             'b)': 6,\n",
       "             'b))': 1,\n",
       "             'b:': 2,\n",
       "             'be': 6,\n",
       "             'bin/spark-shell': 1,\n",
       "             'by': 4,\n",
       "             'cache': 1,\n",
       "             'cache.': 1,\n",
       "             'cached:': 1,\n",
       "             'call': 1,\n",
       "             'called': 2,\n",
       "             'can': 12,\n",
       "             'chain': 1,\n",
       "             'cluster,': 2,\n",
       "             'cluster-wide': 1,\n",
       "             'code': 1,\n",
       "             'code,': 1,\n",
       "             'collect': 2,\n",
       "             'collection': 1,\n",
       "             'combined': 1,\n",
       "             'common': 1,\n",
       "             'complete': 1,\n",
       "             'complex': 1,\n",
       "             'components.': 1,\n",
       "             'computations.': 1,\n",
       "             'compute': 1,\n",
       "             'conf': 1,\n",
       "             'configuration': 1,\n",
       "             'connecting': 1,\n",
       "             'constructor': 1,\n",
       "             'contain': 1,\n",
       "             'containing': 4,\n",
       "             'contains': 1,\n",
       "             'correctly,': 1,\n",
       "             'correctly.': 1,\n",
       "             'count.': 1,\n",
       "             'counts': 3,\n",
       "             'create': 2,\n",
       "             'created': 1,\n",
       "             'creating': 1,\n",
       "             'data': 5,\n",
       "             'dataset': 2,\n",
       "             'declared': 1,\n",
       "             'def': 1,\n",
       "             'define': 1,\n",
       "             'dependency.': 1,\n",
       "             'depends': 2,\n",
       "             'deployment': 1,\n",
       "             'described': 1,\n",
       "             'directly:': 2,\n",
       "             'directory': 3,\n",
       "             'directory:': 2,\n",
       "             'distributed': 1,\n",
       "             'do': 1,\n",
       "             'download': 2,\n",
       "             'earlier': 1,\n",
       "             'easier': 1,\n",
       "             'easily': 1,\n",
       "             'easily:': 1,\n",
       "             'either': 1,\n",
       "             'else': 1,\n",
       "             'elsewhere.': 1,\n",
       "             'even': 1,\n",
       "             'example,': 2,\n",
       "             'examples': 2,\n",
       "             'examples,': 2,\n",
       "             'examples/src/main/python/pi.py': 1,\n",
       "             'examples/src/main/r/dataframe.R': 1,\n",
       "             'existing': 1,\n",
       "             'explains': 1,\n",
       "             'explore': 1,\n",
       "             'extending': 1,\n",
       "             'fact,': 1,\n",
       "             'feature': 1,\n",
       "             'few': 1,\n",
       "             'file': 4,\n",
       "             'file,': 1,\n",
       "             'file.': 2,\n",
       "             'files)': 1,\n",
       "             'filter': 1,\n",
       "             'find': 3,\n",
       "             'first': 4,\n",
       "             'flatMap,': 1,\n",
       "             'flow': 1,\n",
       "             'flows': 1,\n",
       "             'follow': 1,\n",
       "             'following': 1,\n",
       "             'follows:': 1,\n",
       "             'for': 4,\n",
       "             'from': 5,\n",
       "             'function': 2,\n",
       "             'functions': 2,\n",
       "             'good': 1,\n",
       "             'guide': 1,\n",
       "             'guide,': 2,\n",
       "             'guide.': 1,\n",
       "             'have': 1,\n",
       "             'head': 1,\n",
       "             'how': 1,\n",
       "             'hundreds': 1,\n",
       "             'if': 1,\n",
       "             'implement': 1,\n",
       "             'import': 5,\n",
       "             'in': 16,\n",
       "             'in-depth': 1,\n",
       "             'in-memory': 1,\n",
       "             'include': 1,\n",
       "             'includes': 1,\n",
       "             'information': 1,\n",
       "             'initialize': 1,\n",
       "             'initializes': 1,\n",
       "             'installed.': 1,\n",
       "             'instead': 1,\n",
       "             'integer': 1,\n",
       "             'interactive': 1,\n",
       "             'interactively': 1,\n",
       "             'interactively.': 1,\n",
       "             'interesting': 1,\n",
       "             'into': 1,\n",
       "             'introduce': 1,\n",
       "             'introduction': 1,\n",
       "             'is': 11,\n",
       "             'it': 1,\n",
       "             'item': 1,\n",
       "             'items': 3,\n",
       "             'iterative': 1,\n",
       "             'its': 1,\n",
       "             'it’s': 1,\n",
       "             'jar': 1,\n",
       "             'java.lang.Math': 2,\n",
       "             'just': 1,\n",
       "             'language': 1,\n",
       "             'large': 1,\n",
       "             'largest': 1,\n",
       "             'layout': 2,\n",
       "             'learn': 1,\n",
       "             'let’s': 2,\n",
       "             'libraries)': 1,\n",
       "             'library.': 1,\n",
       "             'libraryDependencies': 1,\n",
       "             'like': 2,\n",
       "             'line': 3,\n",
       "             'line.contains(\"Spark\"))': 1,\n",
       "             'line.contains(\"Spark\")).count()': 1,\n",
       "             'line.contains(\"a\")).count()': 1,\n",
       "             'line.contains(\"b\")).count()': 1,\n",
       "             'line.split(\"': 3,\n",
       "             'lines': 2,\n",
       "             'linesWithSpark': 2,\n",
       "             'linesWithSpark.cache()': 1,\n",
       "             'linesWithSpark.count()': 2,\n",
       "             'linesWithSpark:': 1,\n",
       "             'literals': 1,\n",
       "             'local[4]': 1,\n",
       "             'location': 1,\n",
       "             'logData': 1,\n",
       "             'logData.filter(line': 2,\n",
       "             'logFile': 1,\n",
       "             'look': 1,\n",
       "             'main()': 1,\n",
       "             'main(args:': 1,\n",
       "             'make': 2,\n",
       "             'many': 1,\n",
       "             'map': 1,\n",
       "             'map,': 1,\n",
       "             'maps': 1,\n",
       "             'mark': 1,\n",
       "             'may': 2,\n",
       "             'menu': 1,\n",
       "             'method': 1,\n",
       "             'more': 2,\n",
       "             'most': 1,\n",
       "             'name': 1,\n",
       "             'named': 1,\n",
       "             'need': 2,\n",
       "             'new': 6,\n",
       "             'nodes.': 1,\n",
       "             'not': 1,\n",
       "             'numAs': 1,\n",
       "             'numBs': 1,\n",
       "             'numBs))': 1,\n",
       "             'number': 2,\n",
       "             'object': 2,\n",
       "             'of': 13,\n",
       "             'on': 9,\n",
       "             'on:': 1,\n",
       "             'or': 7,\n",
       "             'org.apache.spark.SparkConf': 1,\n",
       "             'org.apache.spark.SparkContext': 1,\n",
       "             'org.apache.spark.SparkContext._': 1,\n",
       "             'other': 2,\n",
       "             'our': 4,\n",
       "             'overview': 1,\n",
       "             'overview.': 1,\n",
       "             'own': 1,\n",
       "             'package': 3,\n",
       "             'packaged': 1,\n",
       "             'pairs.': 1,\n",
       "             'part': 2,\n",
       "             'pass': 1,\n",
       "             'pattern': 1,\n",
       "             'per-word': 1,\n",
       "             'place,': 1,\n",
       "             'pointers': 1,\n",
       "             'popularized': 1,\n",
       "             'powerful': 1,\n",
       "             'primary': 1,\n",
       "             'println(\"Lines': 1,\n",
       "             'program': 1,\n",
       "             'program.': 2,\n",
       "             'programming': 3,\n",
       "             'provides': 2,\n",
       "             'pulling': 1,\n",
       "             'querying': 1,\n",
       "             'quick': 1,\n",
       "             'reduce': 2,\n",
       "             'reduceByKey': 1,\n",
       "             'reference.': 1,\n",
       "             'release': 1,\n",
       "             'repeatedly,': 1,\n",
       "             'replace': 1,\n",
       "             'repository': 1,\n",
       "             'res0:': 1,\n",
       "             'res1:': 1,\n",
       "             'res3:': 1,\n",
       "             'res4:': 1,\n",
       "             'res5:': 1,\n",
       "             'res6:': 1,\n",
       "             'res7:': 1,\n",
       "             'res8:': 1,\n",
       "             'res9:': 1,\n",
       "             'return': 3,\n",
       "             'run': 3,\n",
       "             'run-example:': 1,\n",
       "             'running': 4,\n",
       "             'runs': 1,\n",
       "             'same': 1,\n",
       "             'samples': 1,\n",
       "             'say': 1,\n",
       "             'sbt': 3,\n",
       "             'sbt),': 1,\n",
       "             'sc': 1,\n",
       "             'sc.textFile(\"README.md\")': 1,\n",
       "             'sc.textFile(logFile,': 1,\n",
       "             'scala.App': 1,\n",
       "             'scala.App.': 1,\n",
       "             'scala>': 13,\n",
       "             'scalaVersion': 1,\n",
       "             'script': 1,\n",
       "             'see': 1,\n",
       "             'seem': 1,\n",
       "             'self-contained': 1,\n",
       "             'sets': 1,\n",
       "             'sets,': 1,\n",
       "             'several': 1,\n",
       "             'shell': 2,\n",
       "             'shell,': 2,\n",
       "             'should': 2,\n",
       "             'show': 1,\n",
       "             'silly': 1,\n",
       "             'simple': 4,\n",
       "             'simple,': 1,\n",
       "             'simple.sbt': 1,\n",
       "             'simple.sbt,': 1,\n",
       "             'small': 1,\n",
       "             'so': 1,\n",
       "             'some': 1,\n",
       "             'source': 1,\n",
       "             'spark-submit': 4,\n",
       "             'spark.FilteredRDD@17e51082': 1,\n",
       "             'spark.FilteredRDD@7dd4af09': 1,\n",
       "             'spark.MappedRDD@2ee9b6e3': 1,\n",
       "             'spark.RDD[(String,': 1,\n",
       "             'spark.RDD[String]': 3,\n",
       "             'spark.ShuffledAggregatedRDD@71f027b8': 1,\n",
       "             'start': 2,\n",
       "             'striped': 1,\n",
       "             'structure.': 1,\n",
       "             'subset': 1,\n",
       "             'such': 1,\n",
       "             'supports': 1,\n",
       "             'system': 1,\n",
       "             'target/scala-2.10/simple-project_2.10-1.0.jar': 1,\n",
       "             'tens': 1,\n",
       "             'text': 2,\n",
       "             'textFile': 1,\n",
       "             'textFile.count()': 1,\n",
       "             'textFile.filter(line': 2,\n",
       "             'textFile.first()': 1,\n",
       "             'textFile.flatMap(line': 1,\n",
       "             'textFile.map(line': 2,\n",
       "             'textFile:': 1,\n",
       "             'that': 8,\n",
       "             'the': 41,\n",
       "             'them': 1,\n",
       "             'then': 2,\n",
       "             'these': 1,\n",
       "             'they': 1,\n",
       "             'this': 6,\n",
       "             'through': 2,\n",
       "             'thus': 1,\n",
       "             'to': 28,\n",
       "             'together': 1,\n",
       "             'tool': 1,\n",
       "             'transformation': 1,\n",
       "             'transformation.': 1,\n",
       "             'transformations': 3,\n",
       "             'transformations,': 1,\n",
       "             'transforming': 1,\n",
       "             'tutorial': 1,\n",
       "             'typical': 1,\n",
       "             'understand:': 1,\n",
       "             'use': 11,\n",
       "             'used': 2,\n",
       "             'useful': 1,\n",
       "             'using': 3,\n",
       "             'val': 9,\n",
       "             'value,': 1,\n",
       "             'values,': 1,\n",
       "             'version': 2,\n",
       "             'very': 3,\n",
       "             'walk': 1,\n",
       "             'want': 1,\n",
       "             'way': 2,\n",
       "             'we': 8,\n",
       "             'website.': 1,\n",
       "             'well': 1,\n",
       "             'we’ll': 2,\n",
       "             'when': 4,\n",
       "             'where': 1,\n",
       "             'which': 5,\n",
       "             'will': 3,\n",
       "             'wish': 1,\n",
       "             'with': 13,\n",
       "             'won’t': 1,\n",
       "             'word': 1,\n",
       "             'wordCounts': 1,\n",
       "             'wordCounts.collect()': 1,\n",
       "             'wordCounts:': 1,\n",
       "             'words:': 1,\n",
       "             'work': 2,\n",
       "             'write': 2,\n",
       "             'you': 1,\n",
       "             'your': 4,\n",
       "             'you’ll': 1,\n",
       "             '{': 2,\n",
       "             '{..}/{..}/target/scala-2.10/simple-project_2.10-1.0.jar': 1,\n",
       "             '}': 2,\n",
       "             '‘a’': 1,\n",
       "             '‘b’': 1,\n",
       "             '“Programming': 1,\n",
       "             '“hot”': 1})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordCount \n",
    "import os\n",
    "words = sc.textFile(\"file://\" + os.path.abspath(\".\") + \"/quickstart.txt\").flatMap(lambda line : line.split(\" \"))\n",
    "wordCount = words.countByValue()\n",
    "# wordCount = words.map(lambda x: (x,1)).reduceByKey(lambda x,y : x + y)\n",
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('打招呼', ['你好啊~~吃了吗??']), ('约', ['约吗美女?'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRDD = sc.parallelize([\"打招呼 你好啊~~吃了吗??\",\"约 约吗美女?\"])\n",
    "pairRDD = rawRDD.map(lambda line : (line.split(' ')[0],line.split(' ')[1:]))\n",
    "pairRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1069162e8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10694a400>)),\n",
       " (1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x106916048>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10694acf8>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1069160b8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10694afd0>))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PairRDD转化操作\n",
    "pairRDD = sc.parallelize([[1,2],[3,4],[3,6],[4,6]])\n",
    "# 1.reduceByKey 合并相同Key的值\n",
    "pair1 = pairRDD.reduceByKey(lambda x,y : x + y)\n",
    "# 2.groupByKey  对相同Key的值分组\n",
    "pair2 = pairRDD.groupByKey()\n",
    "# 3.mapValues   对RDD中的每个值应用一个函数而不改变键\n",
    "pair3 = pairRDD.mapValues(lambda x : x + 5)\n",
    "# 4.flatMapValues 对RDD中的每个值应用一个返回迭代器的函数，对于每个元素都生成一个对应原键的键值对记录。\n",
    "pair4 = pairRDD.flatMapValues(lambda x: (range(x)))\n",
    "# 5.keys 返回Key的RDD\n",
    "keysRDD = pairRDD.keys()\n",
    "# 6.values 返回value的RDD\n",
    "valuesRDD = pairRDD.values()\n",
    "# 7.sortByKey 按键排序\n",
    "sortedRDD = pairRDD.sortByKey()\n",
    "sortedRDD.collect()\n",
    "# 针对两个RDD的转换操作\n",
    "rdd1 = sc.parallelize([[1,2],[3,4],[3,6]])\n",
    "rdd2 = sc.parallelize([[3,9],[4,5]])\n",
    "# 1.substractByKey  删掉key值重复的元素\n",
    "subRDD = rdd1.subtractByKey(rdd2)\n",
    "# 2.join  内连接\n",
    "joinRDD = rdd1.join(rdd2)\n",
    "# 3.rightOuterJoin 确保第2个RDD的键必须存在   右外连接\n",
    "rightOuterRDD = rdd1.rightOuterJoin(rdd2)\n",
    "# 4.leftOuterJoin 确保第1个RDD的键必须存在  左外连接\n",
    "leftOuterRDD = rdd1.leftOuterJoin(rdd2)\n",
    "# 5.cogroup 将两个RDD中拥有相同键的数据分组到一起\n",
    "coRDD = rdd1.cogroup(rdd2)\n",
    "# joinRDD.collect()\n",
    "# rightOuterRDD.collect()\n",
    "# leftOuterRDD.collect()\n",
    "# coRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Interactive', 'Interactive Analysis with the Spark Shell'),\n",
       " ('Self-Contained', 'Self-Contained Applications'),\n",
       " ('Interactive', 'Interactive Analysis with the Spark Shell'),\n",
       " ('./bin/spark-shell', './bin/spark-shell'),\n",
       " ('textFile:', 'textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3'),\n",
       " ('linesWithSpark:',\n",
       "  'linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af09'),\n",
       " ('wordCounts:',\n",
       "  'wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8'),\n",
       " ('Self-Contained', 'Self-Contained Applications'),\n",
       " ('scalaVersion', 'scalaVersion := \"2.10.5\"'),\n",
       " ('libraryDependencies',\n",
       "  'libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.2\"'),\n",
       " ('./simple.sbt', './simple.sbt'),\n",
       " ('./src/main', './src/main'),\n",
       " ('./src/main/scala', './src/main/scala'),\n",
       " ('./src/main/scala/SimpleApp.scala', './src/main/scala/SimpleApp.scala'),\n",
       " ('Congratulations',\n",
       "  'Congratulations on running your first Spark application!'),\n",
       " ('Finally,',\n",
       "  'Finally, Spark includes several samples in the examples directory (Scala, Java, Python, R). You can run them as follows:'),\n",
       " ('./bin/run-example', './bin/run-example SparkPi'),\n",
       " ('./bin/spark-submit', './bin/spark-submit examples/src/main/python/pi.py'),\n",
       " ('./bin/spark-submit', './bin/spark-submit examples/src/main/r/dataframe.R')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对pairRDD的value进行筛选\n",
    "lines = sc.textFile(\"file://\" + os.path.abspath(\".\")+ \"/quickstart.txt\")\n",
    "pairsRDD = lines.map(lambda line : (line.split(\" \")[0],line))\n",
    "limitLengthRDD = pairsRDD.filter(lambda keyValue : len(keyValue[0]) > 7)\n",
    "limitLengthRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1.5), ('b', 1.0), ('c', 6.5)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 聚合操作   按key值计算平均值\n",
    "pairRDD = sc.parallelize([['a',1],['b',2],['c',3],['a',2],['b',0],['c',10]])\n",
    "aveRDD = pairRDD.mapValues(lambda x:(x,1)).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).mapValues(lambda x:x[0]/x[1])\n",
    "sumRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount = pairRDD.combineByKey((lambda x:(x,1)),\n",
    "                               (lambda x,y:(x[0]+y,x[1]+1)),\n",
    "                               (lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "sumCount.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

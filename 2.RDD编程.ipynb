{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RDD编程\n",
    "### 1.RDD基础\n",
    "RDD是一个不可变的分布式对象集合。每个RDD存在多个分区，这些分区运行在急群中不同的节点上\n",
    "创建RDD的两种方法\n",
    "- 读取外部数据集\n",
    "- 分发驱动器程序对象集合(list/set)\n",
    "\n",
    "RDD支持两种操作\n",
    "- 转化操作  transformation\n",
    "- 行动操作  action\n",
    "\n",
    "\n",
    "**\n",
    "转换操作和行动操作的区别在于Spark计算RDD的方式不同。\n",
    "Spark会惰性计算这些RDD。它们只有在第一次在一个action操作中用到时才会真正的计算。\n",
    "比如textFile()时并不会读取这个文档，而当调用first时，才会计算，而且到第一个满足条件即停止计算，并不会读取整个文档。\n",
    "**\n",
    "\n",
    "Spark的RDD会在每次action时重新计算，如果想复用需要使用__RDD.persist()__让Spark把这个RDD缓存起来。\n",
    "\n",
    "__Spark程序或Shell回话都按如下方式工作__\n",
    "1. 从外部数据创建出输入RDD\n",
    "2. 使用诸如filter() 这样的转化操作对RDD进行转化，以定义新的RDD\n",
    "3. 告诉Spark对需要被重用的中间结果RDD执行persist()操作\n",
    "4. 使用action操作(例如count() first()等) 来触发一次并行计算，Spark会对计算进行优化后再执行\n",
    "\n",
    "_Remark:cache() 和 persist() 是一样的_\n",
    "\n",
    "---\n",
    "### 2.创建RDD\n",
    "创建RDD最简单的方式：SparkContext.parallelize()   将整个数据集先放在一台机器的内存中\n",
    "### 3.RDD操作\n",
    "一般transformation操作会返回**新的RDD**，而action操作会返回**其他的数据类型**。\n",
    "\n",
    "**转化操作**\n",
    "```\n",
    "inputRDD = sc.textFile(path)\n",
    "pythonRDD = inputRDD.filter(lambda x : \"python\" in x)\n",
    "scalaRDD = inputRDD.filter(lambda x : \"Scala\" in x)\n",
    "PSRDD = pythonRDD.union(scalaRDD)\n",
    "PSRDD.collect()\n",
    "```\n",
    "\n",
    "**行动操作**    \n",
    "action操作会强制执行求值用到的RDD转换操作。\n",
    "take()获取RDD中少量的元素，而collect()获取整个RDD的数据，仅在小规模数据集上适用，只有在单机内存范围内才能使用collect()，不能用在大规模数据集上。\n",
    "```\n",
    "print(\"There a \",PSRDD.count(),\" rows data with python or scala\")\n",
    "print(\"Let's take some elements to see:\\n\")\n",
    "for line in PSRDD.take(5):\n",
    "    print(line)\n",
    "```\n",
    "**注意：每当调用一个新的action操作时，整个RDD都会从头开始计算。要规避这种低效行为，需要将中间结果持久化。**\n",
    "**惰性求值**\n",
    "当我们对RDD调用转化操作（例如调用map()）时，操作不会立即执行。    \n",
    "Spark会记录所要求执行的操作的相关信息。   \n",
    "RDD并不是存储着特定数据的数据集，而是通过转化操作构建出来的、记录如何计算数据的指令列表。   \n",
    "我们可以利用action操作来强制Spark执行RDD转化操作，比如count()。这事一种对程序进行测试的简单方法。\n",
    "\n",
    "### 4.向Spark传递函数\n",
    "- 如果函数较短，可以传递lambda表达式\n",
    "- 也可以传递定义的函数\n",
    "- rdd.filter() 定义  Return a new RDD containing only the elements that satisfy a predicate.\n",
    "\n",
    "**注意**   \n",
    "Python会将函数所在的对象也序列化传出去。当传递的对象是某个对象的一个字段引用时如self.field，Spark会把整个对象发到工作节点上。   \n",
    "这样会传递太多东西，也有可能由于Python不知道如何序列化传输对象导致程序失败。   \n",
    "不要这样做:   \n",
    "```\n",
    "def getMatchesFunctionRef(self,rdd):\n",
    "    return rdd.filter(self.isMaatch)\n",
    "def getMatchesMemberRef(self,rdd):\n",
    "    return rdd.filter(lambda x:self.query in x)\n",
    "```\n",
    "这样做:   \n",
    "```\n",
    "getMatchesNoRef(self,rdd):\n",
    "    query = self.query\n",
    "    rdd.filter(lambda x:query in x)\n",
    "```\n",
    "\n",
    "### 5.常见的转化操作和行动操作\n",
    "__map() 和 flatMap()__  \n",
    "_Spark 中 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象；   \n",
    "而flatMap函数则是两个操作的集合——正是“先映射后扁平化”：   \n",
    "操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象   \n",
    "操作2：最后将所有对象合并为一个对象_   \n",
    "```\n",
    "nList = sc.parallelize([\"Hello World\",\"Hello Spark\",\"Hello Python\"])\n",
    "mapRDD = nList.map(lambda x:x.split(\" \"))\n",
    "flatMapRDD = nList.flatMap(lambda x:x.split(\" \"))\n",
    "mapRDD.collect()\n",
    "输出: [['Hello', 'World'], ['Hello', 'Spark'], ['Hello', 'Python']]\n",
    "flatMapRDD.collect()\n",
    "输出: ['Hello', 'World', 'Hello', 'Spark', 'Hello', 'Python']\n",
    "```\n",
    "__伪集合操作__  \n",
    "RDD.distinct()转化操作生成一个只包含不同元素的新RDD,但是开销很大，因为它需要将所有数据通过网络进行shuffle。\n",
    "\n",
    "__action操作__\n",
    "```\n",
    "# reduce\n",
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "words = sc.parallelize([\"Hello\",\"Niko\",\"Bellic\"])\n",
    "sumNums = nums.reduce(lambda x,y : x + y)\n",
    "sumWords = words.reduce(lambda x,y, : x + y)\n",
    "# fold\n",
    "foldNums = nums.fold(1,lambda x,y : x + y)\n",
    "foldNums\n",
    "# aggregate\n",
    "sumCount = nums.aggregate((0,0),\n",
    "               (lambda acc,value : (acc[0] + value,acc[1] + 1)),\n",
    "               (lambda acc1,acc2 : (acc1[0]+acc2[0],acc1[1]+acc2[1])))\n",
    "sumCount[0] / float(sumCount[1])\n",
    "```\n",
    "collect,count,countByValue,take,top,takeOrdered,taksSample,reduce,fold,aggregate,foreach\n",
    "\n",
    "### 6.持久化\n",
    "使用rdd.persist()来将需要频繁使用的数据持久化到内存中(默认级别)  \n",
    "数据将会以序列化的形式缓存在JVM的堆空间中。    \n",
    "调用rdd.unpersist()可以手动把持久化的RDD从缓存中移除.\n",
    "```\n",
    "from pyspark import StorageLevel\n",
    "input = sc.parallelize([1,2,3,4,5])\n",
    "squaredRDD = input.map(lambda x: x*x)\n",
    "squaredRDD.persist(StorageLevel.DISK_ONLY)\n",
    "print(squaredRDD.count())\n",
    "print(squaredRDD.collect())\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "sc = SparkContext(\"local\",\"RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s interactive shell (in Python or Scala), then show how to write applications in Java, Scala, and Python. See the programming guide for a more complete reference.\n",
      "Spark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python. Start it by running the following in the Spark directory:\n",
      "Python\n",
      "Python\n",
      "res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)\n",
      "Python\n",
      "Suppose we wish to write a self-contained application using the Spark API. We will walk through a simple application in Scala (with sbt), Java (with Maven), and Python.\n",
      "Python\n",
      "Finally, Spark includes several samples in the examples directory (Scala, Java, Python, R). You can run them as follows:\n",
      "# For Python examples, use spark-submit directly:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"file://\" + os.path.abspath(\".\") + \"/quickstart.txt\"\n",
    "lines = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用转化操作filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pythonLine = lines.filter(lambda line : \"Python\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用行动操作first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s interactive shell (in Python or Scala), then show how to write applications in Java, Scala, and Python. See the programming guide for a more complete reference.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLine.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把RDD持久化到内存中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at collect at <ipython-input-23-e90d135d5a23>:5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLine.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLine.count()  # 不会重新计算，而是调用内存中的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s interactive shell (in Python or Scala), then show how to write applications in Java, Scala, and Python. See the programming guide for a more complete reference.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLine.first() # 不会重新计算，而是调用内存中的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['panda', 'i like pandas']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建RDD最简单的方式\n",
    "lines = sc.parallelize([\"panda\",\"i like pandas\"])\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bin/spark-submit examples/src/main/python/pi.py',\n",
       " 'This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s interactive shell (in Python or Scala), then show how to write applications in Java, Scala, and Python. See the programming guide for a more complete reference.',\n",
       " 'Spark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python. Start it by running the following in the Spark directory:',\n",
       " 'Scala',\n",
       " 'Scala',\n",
       " 'This first maps a line to an integer value, creating a new RDD. reduce is called on that RDD to find the largest line count. The arguments to map and reduce are Scala function literals (closures), and can use any language feature or Scala/Java library. For example, we can easily call functions declared elsewhere. We’ll use Math.max() function to make this code easier to understand:',\n",
       " 'Scala',\n",
       " 'Suppose we wish to write a self-contained application using the Spark API. We will walk through a simple application in Scala (with sbt), Java (with Maven), and Python.',\n",
       " 'Scala',\n",
       " 'We’ll create a very simple Spark application in Scala–so simple, in fact, that it’s named SimpleApp.scala:',\n",
       " 'Finally, Spark includes several samples in the examples directory (Scala, Java, Python, R). You can run them as follows:',\n",
       " '# For Scala and Java, use run-example:']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化操作\n",
    "inputRDD = sc.textFile(path)\n",
    "pythonRDD = inputRDD.filter(lambda x : \"python\" in x)\n",
    "scalaRDD = inputRDD.filter(lambda x : \"Scala\" in x)\n",
    "PSRDD = pythonRDD.union(scalaRDD)\n",
    "PSRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There a  12  rows data with python or scala\n",
      "Let's take some elements to see:\n",
      "\n",
      "./bin/spark-submit examples/src/main/python/pi.py\n",
      "This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s interactive shell (in Python or Scala), then show how to write applications in Java, Scala, and Python. See the programming guide for a more complete reference.\n",
      "Spark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python. Start it by running the following in the Spark directory:\n",
      "Scala\n",
      "Scala\n"
     ]
    }
   ],
   "source": [
    "# 行动操作\n",
    "print(\"There a \",PSRDD.count(),\" rows data with python or scala\")\n",
    "print(\"Let's take some elements to see:\\n\")\n",
    "for line in PSRDD.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向Spark传递函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello NikoBelic , My name is tom! Second line : Hey Nice to meet you'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Hello NikoBelic , My name is tom! Second line : Hey Nice to meet you\"\n",
    "s = sc.parallelize([s])\n",
    "# word = s.filter(lambda x : \"My\" in x)\n",
    "# word.first()\n",
    "\n",
    "def containsName(s):\n",
    "    return \"name\" in s\n",
    "word = s.filter(containsName)\n",
    "word.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'Hello', 'Spark', 'Hello', 'Python']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nList = sc.parallelize([\"Hello World\",\"Hello Spark\",\"Hello Python\"])\n",
    "mapRDD = nList.map(lambda x:x.split(\" \"))\n",
    "flatMapRDD = nList.flatMap(lambda x:x.split(\" \"))\n",
    "mapRDD.collect()\n",
    "flatMapRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coffee', 'coffee'),\n",
       " ('coffee', 'monkey'),\n",
       " ('coffee', 'kitty'),\n",
       " ('panda', 'coffee'),\n",
       " ('panda', 'monkey'),\n",
       " ('panda', 'kitty'),\n",
       " ('monkey', 'coffee'),\n",
       " ('monkey', 'monkey'),\n",
       " ('monkey', 'kitty'),\n",
       " ('tea', 'coffee'),\n",
       " ('tea', 'monkey'),\n",
       " ('tea', 'kitty'),\n",
       " ('coffee', 'coffee'),\n",
       " ('coffee', 'monkey'),\n",
       " ('coffee', 'kitty')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 伪集合操作\n",
    "lines1 = sc.parallelize([\"coffee\",\"panda\",\"monkey\",\"tea\",\"coffee\"])\n",
    "lines2 = sc.parallelize([\"coffee\",\"monkey\",\"kitty\"])\n",
    "# distinct\n",
    "distincetedWords = lines1.distinct()\n",
    "distincetedWords.collect()\n",
    "# union\n",
    "unionWords = lines1.union(lines2)#.distinct()\n",
    "unionWords.collect()\n",
    "# intersection\n",
    "intersectionWords = lines1.intersection(lines2)\n",
    "intersectionWords.collect()\n",
    "# subtract\n",
    "subtractWords = lines1.subtract(lines2)\n",
    "subtractWords.collect()\n",
    "# cartesian\n",
    "cartesianWords = lines1.cartesian(lines2)\n",
    "cartesianWords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action操作   \n",
    "# reduce\n",
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "words = sc.parallelize([\"Hello\",\"Niko\",\"Bellic\"])\n",
    "sumNums = nums.reduce(lambda x,y : x + y)\n",
    "sumWords = words.reduce(lambda x,y, : x + y)\n",
    "# fold\n",
    "foldNums = nums.fold(1,lambda x,y : x + y)\n",
    "foldNums\n",
    "# aggregate\n",
    "sumCount = nums.aggregate((0,0),\n",
    "               (lambda acc,value : (acc[0] + value,acc[1] + 1)),\n",
    "               (lambda acc1,acc2 : (acc1[0]+acc2[0],acc1[1]+acc2[1])))\n",
    "sumCount[0] / float(sumCount[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "# 持久化\n",
    "from pyspark import StorageLevel\n",
    "input = sc.parallelize([1,2,3,4,5])\n",
    "squaredRDD = input.map(lambda x: x*x)\n",
    "squaredRDD.persist(StorageLevel.DISK_ONLY)\n",
    "print(squaredRDD.count())\n",
    "print(squaredRDD.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
